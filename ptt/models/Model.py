# ------------------------------------------------------------------------------
# Class all model definitions should descend from.
# ------------------------------------------------------------------------------

import os
import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from torchsummary  import summary
from collections import OrderedDict

from src.utils.pytorch.pytorch_load_restore import load_model_state, save_model_state

class Model(nn.Module):
    """
    :param input_shape: channels first
    """
    def __init__(self, input_shape=(1, 32, 32), output_dim=2, weights_file_name=None):
        super(Model, self).__init__()
        self.output_dim = output_dim
        self.input_shape = input_shape
        self.weights_file_name = weights_file_name

        # Initialization
        self.initialize(self.weights_file_name)

        # Every subclass of the 'model' class has the following lists defined
        self.layers = OrderedDict() # Definition of all layers
        self.operations_before = dict() # Performed before a layer
        # E.g. x.view(-1, self.num_flat_features(x)
        self.operations_after = dict() # Performed after a layer

    # E.g. pretrained features
    def preprocess_input(self, x):
        return x

    def forward(self, x):
        for layer_name, layer in self.layers.items():
            for operation in self.operations_before.get(layer_name, []):
                x = operation(x)
            x = layer(x)
            for operation in self.operations_after.get(layer_name, []):
                x = operation(x)
        return x

    # Initialization

    def initialize(self, weights_file_name=None):
        """
        Tries to restore a previous model. If no model is found but a file name
        if provided, the model is saved.
        """
        if weights_file_name is not None:
            path = os.path.join('storage', 'agent_states')
            restored = load_model_state(self, weights_file_name, path=path)
            if restored:
                print('Initial parameters {} were restored'.format(weights_file_name))
            else:
                self.xavier_initialize()
                save_model_state(self, name=weights_file_name, path=path)
                print('Initial parameters {} were saved'.format(weights_file_name))
        else:
            self.xavier_initialize()

    def xavier_initialize(self):
        """Xavier initialization. For ReLu, He may be better."""
        modules = [
            m for n, m in self.named_modules() if
            'conv' in n or 'linear' in n
        ]
        parameters = [
            p for
            m in modules for
            p in m.parameters() if
            p.dim() >= 2
        ]
        for p in parameters:
            nn.init.xavier_uniform_(p)

    def get_param_list_static(self):
        """
        Returns a 1D array of parameter values
        """
        model_params_array = []
        for _, param in self.state_dict().items():
            model_params_array.append(param.reshape(-1).cpu().numpy())
        return np.concatenate(model_params_array)

    # Methods to calculate the feature size

    def num_flat_features(self, x):
        """
        Flattened view of all dimensions except the batch size.
        """
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features

    def flatten(self, x):
        return x.view(-1, self.num_flat_features(x))

    def size_before_lin(self, shape_input):
        """ 
        Size after linearization
        returns: integer of dense size
        """
        return shape_input[0]*shape_input[1]*shape_input[2]

    def size_after_conv(self, shape_input, output_channels, kernel):
        """
        Gives the number of output neurons after the conv operation.
        The first dimension is the channel depth and the other 2 are given by
        input volume (size - kernel size + 2*padding)/stride + 1
        param shape_input: (nr_channels, dim_x, dim_y)
        returns: 3D tuple
        """
        return (output_channels, shape_input[1]-kernel+1, shape_input[2]-kernel+1)

    def size_after_pooling(self, shape_input, shape_pooling):
        """
        Maintains the first input dimension, which is the output channels in 
        the previous conv layer. The others are divided by the shape of the 
        pooling.
        """
        return (shape_input[0], shape_input[1]//shape_pooling[0], shape_input[2]//shape_pooling[1])

    # Methods to output model information

    def model_summary(self):
        """Keras-style summary."""
        summary(self, input_size=(3, 299, 299))

    def model_description(self):
        print('Model: {}\n'.format(self.__class__))

        features_size = self.input_shape
        print('Input shape (channels first): {}\n'.format(features_size))

        state_dictionary = self.state_dict()
        for layer_name, layer in self.layers.items():
            layer_type = layer.__class__.__name__
            print('{} layer {}'.format(layer_type, layer_name))
            weights = state_dictionary.get(layer_name+'.weight', None)
            bias = state_dictionary.get(layer_name+'.bias', None)
            if weights is not None and bias is not None:
                print('Shape weights: {}, bias: {}'.format(list(weights.shape), list(bias.shape)))